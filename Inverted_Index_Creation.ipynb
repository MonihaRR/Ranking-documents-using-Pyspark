{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSC502inv_idx.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUgtbVlwZrJ_"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init(\"spark-2.4.4-bin-hadoop2.7\")# SPARK_HOME\n",
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka4IPeo7b9SE",
        "outputId": "634400a5-917a-4c0f-ee93-898f9e461d70"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsWUBAs7cdqw",
        "outputId": "aa97b816-82de-4c58-83db-cf26e1bf4a66"
      },
      "source": [
        "!rm -rf input_docs\n",
        "!cp /content/drive/MyDrive/input_docs_sample.zip . # Sample data \n",
        "#!cp /content/drive/MyDrive/input_docs.zip . (Real data)\n",
        "!unzip input_docs_sample.zip > /dev/null\n",
        "#!unzip input_docs.zip > /dev/null\n",
        "!ls input_docs/ | wc -l"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjVsitwadVch",
        "outputId": "0ab145e1-1755-4c1e-a828-76835cf84025"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bJ-7I5pdbnD"
      },
      "source": [
        "**Create an RDD from a text file**\n",
        "\n",
        "Each line of the text file becomes an element of the RDD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s36LJ548dpmw",
        "outputId": "a7b30248-a61e-4ee5-af8e-f4226a87f63b"
      },
      "source": [
        "# wholeTextFiles generates an RDD of pair values, \n",
        "# where the key is the full path of each file, the value is the content of each file\n",
        "#input = sc.wholeTextFiles(\"/content/drive/My\\ Drive/input_docs\");\n",
        "input = sc.wholeTextFiles(\"input_docs\");\n",
        "\n",
        "# Now we strip the prefix of filenames and leave only the basename. \n",
        "# e.g. 'file:/content/drive/My Drive/Colab Notebooks/data_spark/input_docs/3.html'\n",
        "# becomes '3.html' \n",
        "import os\n",
        "\n",
        "#(did,text)\n",
        "final_input = input.map(lambda x: (int(os.path.basename(x[0]).split(\".\")[0]), x[1]))\n",
        "\n",
        "print(final_input.take(1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(4, '<H2>26-FEB-1987 15:07:13.72</H2>\\r\\n<H2>TALKING POINT/BANKAMERICA BAC EQUITY OFFER</H2>\\r\\nBankAmerica Corp is not under\\npressure to act quickly on its proposed equity offering and\\nwould do well to delay it because of the stock\\'s recent poor\\nperformance, banking analysts said.\\n    Some analysts said they have recommended BankAmerica delay\\nits up to one-billion-dlr equity offering, which has yet to be\\napproved by the Securities and Exchange Commission.\\n    BankAmerica stock fell this week, along with other banking\\nissues, on the news that Brazil has suspended interest payments\\non a large portion of its foreign debt.\\n    The stock traded around 12, down 1/8, this afternoon,\\nafter falling to 11-1/2 earlier this week on the news.\\n    Banking analysts said that with the immediate threat of the\\nFirst Interstate Bancorp I takeover bid gone, BankAmerica is\\nunder no pressure to sell the securities into a market that\\nwill be nervous on bank stocks in the near term.\\n    BankAmerica filed the offer on January 26. It was seen as\\none of the major factors leading the First Interstate\\nwithdrawing its takeover bid on February 9.\\n    A BankAmerica spokesman said SEC approval is taking longer\\nthan expected and market conditions must now be re-evaluated.\\n    \"The circumstances at the time will determine what we do,\"\\nsaid Arthur Miller, BankAmerica\\'s Vice President for Financial\\nCommunications, when asked if BankAmerica would proceed with\\nthe offer immediately after it receives SEC approval.\\n    \"I\\'d put it off as long as they conceivably could,\" said\\nLawrence Cohn, analyst with Merrill Lynch, Pierce, Fenner and\\nSmith.\\n    Cohn said the longer BankAmerica waits, the longer they\\nhave to show the market an improved financial outlook.\\n    Although BankAmerica has yet to specify the types of\\nequities it would offer, most analysts believed a convertible\\npreferred stock would encompass at least part of it.\\n    Such an offering at a depressed stock price would mean a\\nlower conversion price and more dilution to BankAmerica stock\\nholders, noted Daniel Williams, analyst with Sutro Group.\\n    Several analysts said that while they believe the Brazilian\\ndebt problem will continue to hang over the banking industry\\nthrough the quarter, the initial shock reaction is likely to\\nease over the coming weeks.\\n    Nevertheless, BankAmerica, which holds about 2.70 billion\\ndlrs in Brazilian loans, stands to lose 15-20 mln dlrs if the\\ninterest rate is reduced on the debt, and as much as 200 mln\\ndlrs if Brazil pays no interest for a year, said Joseph\\nArsenio, analyst with Birr, Wilson and Co.\\n    He noted, however, that any potential losses would not show\\nup in the current quarter.\\n    With other major banks standing to lose even more than\\nBankAmerica if Brazil fails to service its debt, the analysts\\nsaid they expect the debt will be restructured, similar to way\\nMexico\\'s debt was, minimizing losses to the creditor banks.\\n \\n ')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoXyZeIthagn",
        "outputId": "3ca205f2-596b-4a17-89e0-7bae396865c7"
      },
      "source": [
        "nltk.download('punkt')\n",
        "from bs4 import BeautifulSoup\n",
        "import collections\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "stem = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76IA3G1EfZ5l",
        "outputId": "befe4113-b84e-4c0d-f0a7-5ad57fab5ec6"
      },
      "source": [
        "# Doc to wordlist function\n",
        "# The output will be a list of tuples such as \n",
        "# (\"apple\", (3,10,10/20)), \n",
        "# where 3 is docid, \n",
        "# 10 is frequency of \"apple\" in this doc, \n",
        "# 20 is maxf in in this doc.\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def dw(docid, htmltext):\n",
        "\n",
        "  lower_html = htmltext.lower() #convert into lowercase\n",
        "  clean_text = BeautifulSoup(lower_html).get_text()\n",
        "  \n",
        "  clean_text = re.sub(\"\\d+\", \" \", clean_text) #remove digits\n",
        "  clean_text = re.sub('[^A-Za-z0-9]+', ' ', clean_text) #remove special characters\n",
        "  clean_text = re.sub(\"\\s+\", \" \", clean_text) #remove extra spaces\n",
        "  \n",
        "  clean_text = word_tokenize(clean_text)\n",
        "  clean_text = [word for word in clean_text if word not in stopwords.words('english')]\n",
        "  \n",
        "  output = [];\n",
        "  counter = Counter(clean_text)\n",
        "  maxf = Counter(clean_text).most_common(1)[0][1]\n",
        "  for ele in counter:\n",
        "    tuple1 = (ele);\n",
        "    tuple2 = (int(docid),counter[ele],counter[ele]/maxf)\n",
        "    final_tuple = (tuple1, tuple2)\n",
        "    output.append(final_tuple);\n",
        "  return(output)\n",
        "\n",
        "word_docid_freq_tf = final_input.flatMap(lambda x: dw(x[0],x[1]))\n",
        "print(word_docid_freq_tf.take(2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('feb', (4, 1, 0.07142857142857142)), ('talking', (4, 1, 0.07142857142857142))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrHzuJcoxQ97",
        "outputId": "00660b95-321f-4002-fe28-0a3404d37784"
      },
      "source": [
        "%xmode Verbose\n",
        "#combining all the words and documents ids frequency\n",
        "word_list_freq_tf = word_docid_freq_tf.map(lambda x: (x[0],[x[1]])).reduceByKey(lambda x,y: x+y)\n",
        "# Now create an RDD as follows \n",
        "# (word, [(did1,freq1,tf1), (did2,freq2,tf2), ...])\n",
        "word_postinglist_freq_tf = sc.parallelize(word_list_freq_tf.collect())\n",
        "#print(word_postinglist_freq_tf.count())\n",
        "print(word_postinglist_freq_tf.map(lambda x : (x[0], list(x[1]))).take(1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exception reporting mode: Verbose\n",
            "[('feb', [(4, 1, 0.07142857142857142), (5, 1, 0.16666666666666666), (1, 1, 0.07142857142857142), (2, 1, 0.2), (3, 1, 0.3333333333333333)])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNd_P1qi27qo",
        "outputId": "99f315bf-baa4-42fc-c5f3-aec3e9c67828"
      },
      "source": [
        "# idf = 1/len(postinglist_tf)\n",
        "%xmode Verbose\n",
        "%time\n",
        "import math\n",
        "from decimal import Decimal\n",
        "with_tfidf = []\n",
        "for row in word_postinglist_freq_tf.collect():\n",
        "  idf = 1/len(row[1]) #present in number of documents\n",
        "  backto_tuple = []\n",
        "  for eachrow in row[1]:\n",
        "    list_row = list(eachrow) #convert to list\n",
        "    tfidf_val = Decimal((list_row[2]*idf)) #tf*idf\n",
        "    list_row[2] =\"{:.18f}\".format(float(tfidf_val))\n",
        "    backto_tuple.append(tuple(list_row))\n",
        "\n",
        "  eachtuple = (row[0], backto_tuple)\n",
        "  with_tfidf.append(eachtuple);\n",
        "# print(with_tfidf[0])\n",
        "# creating a RDD\n",
        "word_postinglist_freq_tfidf = sc.parallelize(with_tfidf)\n",
        "\n",
        "print(word_postinglist_freq_tfidf.take(2))\n",
        "#print(word_postinglist_freq_tfidf.count())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exception reporting mode: Verbose\n",
            "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
            "Wall time: 6.44 µs\n",
            "[('feb', [(4, 1, '0.014285714285714285'), (5, 1, '0.033333333333333333'), (1, 1, '0.014285714285714285'), (2, 1, '0.040000000000000008'), (3, 1, '0.066666666666666666')]), ('talking', [(4, 1, '0.071428571428571425')])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqc7sZ7z1DYH"
      },
      "source": [
        "\n",
        "**Calculating Magnitude**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQBv_2NH32gg",
        "outputId": "c7b76ca0-9692-460a-d539-b00897b72a6e"
      },
      "source": [
        "# Now, we would like to obtain the magnitude of each doc.\n",
        "# First, produce (did, (freq,tfidf)) for each word of doc did; \n",
        "# We do don't need the word itself, just its (freq,tfidf). \n",
        "# Then, do reduceByKey on these tuples and obtain maxfreq and \n",
        "# magnitude (squared) for each document. \n",
        "%xmode Verbose\n",
        "import math\n",
        "from decimal import Decimal\n",
        "\n",
        "convert_to_list = word_postinglist_freq_tfidf.collect() #make a list\n",
        "convert_to_list = list(map(list, convert_to_list)) \n",
        "new_list = []\n",
        "for val in convert_to_list: \n",
        "  for row in val[1]:\n",
        "    row = list(row)\n",
        "    # print(type(row[2]))\n",
        "    magnitude = Decimal((float(row[2])**2))\n",
        "    magnitude =\"{:.18f}\".format(float(magnitude))\n",
        "    row = (row[0],(row[1],magnitude))\n",
        "  new_list.append(row)\n",
        "\n",
        "# RDD of (did,(freq,tfidf)) tuples\n",
        "# creating a RDD\n",
        "did_freq_tfidfsq_rdd = sc.parallelize(new_list)\n",
        "\n",
        "print(did_freq_tfidfsq_rdd.take(2))\n",
        "new_list1 = did_freq_tfidfsq_rdd.reduceByKey(lambda x,y: (max(x[0],y[0]),(float(x[1])+float(y[1]))))\n",
        "\n",
        "# Produce (did,(maxf,magnitudesq))\n",
        "# creating a RDD\n",
        "doc_maxf_mag = sc.parallelize(new_list1.collect())\n",
        "\n",
        "#print(doc_maxf_mag.count())\n",
        "print(doc_maxf_mag.take(2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exception reporting mode: Verbose\n",
            "[(3, (1, '0.004444444444444444')), (4, (1, '0.005102040816326530'))]\n",
            "[(4, (14, 2.7653061224489752)), (2, (5, 3.8700000000000014))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpWD8pvv4IA2"
      },
      "source": [
        "!rm -rf inv_idx\n",
        "word_postinglist_freq_tfidf.saveAsTextFile(\"inv_idx\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0lr0KzP4N7A"
      },
      "source": [
        "!rm -rf doc_mag\n",
        "doc_maxf_mag.saveAsTextFile(\"doc_mag\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFOjSI8k4TJl",
        "outputId": "55d75689-bdf2-4499-defd-4205bb9e90e5"
      },
      "source": [
        "!ls -lrt inv_idx\n",
        "!head inv_idx/part-00001\n",
        "!wc -l inv_idx/part-00000\n",
        "!wc -l inv_idx/part-00001\n",
        "!cat inv_idx/part-00000 inv_idx/part-00001 > /content/drive/My\\ Drive/inv_idx.txt\n",
        "!wc -l /content/drive/My\\ Drive/inv_idx.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 24\n",
            "-rw-r--r-- 1 root root  9670 Apr 12 19:39 part-00001\n",
            "-rw-r--r-- 1 root root 10042 Apr 12 19:39 part-00000\n",
            "-rw-r--r-- 1 root root     0 Apr 12 19:39 _SUCCESS\n",
            "('inc', [(2, 1, '0.100000000000000006'), (3, 1, '0.166666666666666657')])\n",
            "('plan', [(2, 1, '0.100000000000000006'), (3, 1, '0.166666666666666657')])\n",
            "('money', [(2, 1, '0.200000000000000011')])\n",
            "('activities', [(2, 1, '0.200000000000000011')])\n",
            "('companies', [(2, 1, '0.200000000000000011')])\n",
            "('subsidiary', [(2, 1, '0.200000000000000011')])\n",
            "('plc', [(2, 1, '0.200000000000000011')])\n",
            "('owns', [(2, 1, '0.200000000000000011')])\n",
            "('pct', [(2, 1, '0.200000000000000011')])\n",
            "('operated', [(2, 1, '0.200000000000000011')])\n",
            "198 inv_idx/part-00000\n",
            "198 inv_idx/part-00001\n",
            "396 /content/drive/My Drive/inv_idx.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90sFBzj34fd3",
        "outputId": "09bb9d3f-79db-44d0-857f-8cccacfaed2f"
      },
      "source": [
        "!ls -lrt doc_mag\n",
        "!head doc_mag/part-00000\n",
        "!wc -l doc_mag/part-00000\n",
        "!wc -l doc_mag/part-00001\n",
        "!cat doc_mag/part-00000 doc_mag/part-00001 > /content/drive/My\\ Drive/doc_mag.txt\n",
        "!wc -l /content/drive/My\\ Drive/doc_mag.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 8\n",
            "-rw-r--r-- 1 root root 59 Apr 12 19:39 part-00000\n",
            "-rw-r--r-- 1 root root 84 Apr 12 19:39 part-00001\n",
            "-rw-r--r-- 1 root root  0 Apr 12 19:39 _SUCCESS\n",
            "(4, (14, 2.7653061224489752))\n",
            "(2, (5, 3.8700000000000014))\n",
            "2 doc_mag/part-00000\n",
            "3 doc_mag/part-00001\n",
            "5 /content/drive/My Drive/doc_mag.txt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}